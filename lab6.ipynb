{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.replace(r\"&AMP\", ' ', regex=True)\n",
    "data['text'] = data['text'].str.replace(r\"&GT\", ' ', regex=True)\n",
    "data['text'] = data['text'].str.replace(r\"@+\", '', regex=True)\n",
    "data['text'] = data['text'].str.replace(\"?\", '? ')\n",
    "data['text'] = data['text'].str.replace(\"!\", '! ')\n",
    "data['text'] = data['text'].str.replace(r\"HTTP\\S+\", '', regex=True)\n",
    "data['text'] = data['text'].str.replace(r\"[^A-Z0-9 ]\", ' ', regex=True)\n",
    "data['text'] = data['text'].str.replace(r\"\\d+(?=[A-Z])\", '', regex=True)\n",
    "data['text'] = data['text'].str.replace(\"Á\", 'A', regex=True)\n",
    "data['text'] = data['text'].str.replace(\"É\", 'E', regex=True)\n",
    "data['text'] = data['text'].str.replace(\"Í\", 'I', regex=True)\n",
    "data['text'] = data['text'].str.replace(\"Ó\", 'O', regex=True)\n",
    "data['text'] = data['text'].str.replace(\"Ú\", 'U', regex=True)\n",
    "data['text'] = data['text'].str.replace(r\" +\", ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['location'] = data['location'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['location'] = data['location'].str.replace(r\"[^A-Z0-9 ]\", '', regex=True)\n",
    "data['location'] = data['location'].str.replace(r\" +\", ' ', regex=True)\n",
    "data['location'] = data['location'].replace(\" \", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['keyword'] = data['keyword'].str.upper()\n",
    "data['keyword'] = data['keyword'].str.replace(\"%20\", ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in data:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets = data[data['target'] == 1]['text'].tolist()\n",
    "non_disaster_tweets = data[data['target'] == 0]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def get_word_freq(tweets):\n",
    "    words = []\n",
    "    for tweet in tweets:\n",
    "        words.extend([word.lower() for word in word_tokenize(tweet) if word.isalnum() and word.lower() not in stop_words])\n",
    "    return FreqDist(words)\n",
    "\n",
    "# Assuming disaster_tweets and non_disaster_tweets are defined\n",
    "disaster_freq = get_word_freq(disaster_tweets)\n",
    "non_disaster_freq = get_word_freq(non_disaster_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_common = disaster_freq.most_common(20)\n",
    "non_disaster_common = non_disaster_freq.most_common(20)\n",
    "\n",
    "print(\"Palabras más comunes en tweets de desastres:\")\n",
    "print(disaster_common)\n",
    "print(\"\\nPalabras más comunes en tweets que no son de desastres:\")\n",
    "print(non_disaster_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "\n",
    "def get_ngram_freq(tweets, n):\n",
    "    ngrams = []\n",
    "    for tweet in tweets:\n",
    "        tokens = [word.lower() for word in word_tokenize(tweet) if word.isalnum() and word.lower() not in stop_words]\n",
    "        if n == 2:\n",
    "            ngrams.extend(list(bigrams(tokens)))\n",
    "        elif n == 3:\n",
    "            ngrams.extend(list(trigrams(tokens)))\n",
    "    return FreqDist(ngrams)\n",
    "\n",
    "disaster_bigrams = get_ngram_freq(disaster_tweets, 2)\n",
    "disaster_trigrams = get_ngram_freq(disaster_tweets, 3)\n",
    "\n",
    "print(\"\\nBigramas más comunes en tweets de desastres:\")\n",
    "print(disaster_bigrams.most_common(10))\n",
    "print(\"\\nTrigramas más comunes en tweets de desastres:\")\n",
    "print(disaster_trigrams.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine disaster and non-disaster frequencies into a single dataframe\n",
    "disaster_freq_df = pd.DataFrame(disaster_freq.most_common(20), columns=['word', 'frequency'])\n",
    "disaster_freq_df['type'] = 'disaster'\n",
    "non_disaster_freq_df = pd.DataFrame(non_disaster_freq.most_common(20), columns=['word', 'frequency'])\n",
    "non_disaster_freq_df['type'] = 'non_disaster'\n",
    "combined_freq_df = pd.concat([disaster_freq_df, non_disaster_freq_df])\n",
    "combined_freq_df.reset_index(drop=True, inplace=True)\n",
    "combined_freq_df.sort_values('frequency', ascending=False, inplace=True)\n",
    "combined_freq_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for disaster tweets\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "disaster_wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(disaster_freq)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(disaster_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word cloud for disaster tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for non-disaster tweets\n",
    "non_disaster_wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(non_disaster_freq)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(non_disaster_wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word cloud for non-disaster tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in disaster and non-disaster tweets histogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "sb.barplot(x='word', y='frequency', hue='type', data=combined_freq_df)\n",
    "plt.title('Most common words in disaster and non-disaster tweets')\n",
    "plt.xticks(rotation=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinar palabras positivas, negativas y neutrales en los tweets\n",
    "positive_words = ['like', 'love', 'good', 'great', 'lol']\n",
    "negative_words = ['disaster', 'suicide', 'bad', 'ruin', 'fear', 'injured', 'fatal', 'devastated', 'killed', 'kills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>negativity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OUR DEEDS ARE THE REASON OF THIS EARTHQUAKE MA...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FOREST FIRE NEAR LA RONGE SASK CANADA</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ALL RESIDENTS ASKED TO SHELTER IN PLACE ARE BE...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13 000 PEOPLE RECEIVE WILDFIRES EVACUATION ORD...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JUST GOT SENT THIS PHOTO FROM RUBY ALASKA AS S...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TWO GIANT CRANES HOLDING A BRIDGE COLLAPSE INT...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ARIA AHRARY THETAWNIEST THE OUT OF CONTROL WIL...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1 94 01 04 UTC KM S OF VOLCANO HAWAII</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POLICE INVESTIGATING AFTER AN E BIKE COLLIDED ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THE LATEST MORE HOMES RAZED BY NORTHERN CALIFO...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  negativity  \n",
       "0     OUR DEEDS ARE THE REASON OF THIS EARTHQUAKE MA...       1           0  \n",
       "1                 FOREST FIRE NEAR LA RONGE SASK CANADA       1           0  \n",
       "2     ALL RESIDENTS ASKED TO SHELTER IN PLACE ARE BE...       1           0  \n",
       "3     13 000 PEOPLE RECEIVE WILDFIRES EVACUATION ORD...       1           0  \n",
       "4     JUST GOT SENT THIS PHOTO FROM RUBY ALASKA AS S...       1           0  \n",
       "...                                                 ...     ...         ...  \n",
       "7608  TWO GIANT CRANES HOLDING A BRIDGE COLLAPSE INT...       1           0  \n",
       "7609  ARIA AHRARY THETAWNIEST THE OUT OF CONTROL WIL...       1           0  \n",
       "7610            M1 94 01 04 UTC KM S OF VOLCANO HAWAII        1           0  \n",
       "7611  POLICE INVESTIGATING AFTER AN E BIKE COLLIDED ...       1           0  \n",
       "7612  THE LATEST MORE HOMES RAZED BY NORTHERN CALIFO...       1           0  \n",
       "\n",
       "[7613 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweets = pd.DataFrame(columns=['text', 'target', 'score'])\n",
    "tweets['text'] = data['text']\n",
    "tweets['target'] = data['target']\n",
    "\n",
    "def score_tweet(row):\n",
    "    text: str = row[\"text\"].lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if word in positive_words:\n",
    "            score += 1\n",
    "        if word in negative_words:\n",
    "            score -= 1\n",
    "            \n",
    "    return score\n",
    "    \n",
    "tweets['score'] = tweets.apply(score_tweet, axis=1)\n",
    "tweets.sort_values('score', ascending=True, inplace=True)\n",
    "\n",
    "data['negativity'] = tweets['score']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los 10 tweets más positivos:\n",
      "HAVE YOU HEARD 05 LOVE TO LOVE YOU HEAT WAVE VOL 5 BY GEORGE DEEJAYEMPIRESOUND ON SOUNDCLOUD NP  : No desastre\n",
      "ROGUEWATSON NOTHING WRONG WITH THAT THE LETHAL WEAPON SERIES IS GREAT YES THEY RE ALL GREAT  : No desastre\n",
      "GOOD FOR HER LOL  : No desastre\n",
      "BLIZZHEROES WOULD LOVE TO SEE A DIABLO MAP THEMED AFTER PLACES LIKE WESTMARCH OR MT ARREAT OR PANDEMONIUM  : No desastre\n",
      "ZAK BAGANS PETS R LIKE PART OF THE FAMILY I LOVE ANIMALS THE LAST 2 PETS I HAD I RESCUED BREAKS MY HEART WHEN ANIMALS ARE MISTREATED  : No desastre\n",
      "CHECK OUT WANT TWISTER TICKETS AND A VIP EXPERIENCE TO SEE SHANIA CLICK HERE AT I WOULD LOVE LOVE LOVE TO WIN : No desastre\n",
      "LOVE LOVE LOVE DO YOU REMEMBER YOUR FIRST CRUSH  : No desastre\n",
      "LOOKS LIKE A MUDSLIDE AND TASTES LIKE RUBBER OH HOW I LOVE THE BAKE OFF BRITISHBAKEOFF PAULHOLLYWOOD : No desastre\n",
      "I M LIABLE TO SOUND LIKE A WOUNDED ANIMAL DURING SEX IF THE IS GOOD LOL : No desastre\n",
      "KUUALOHAX MORE LIKE YOU LOVE YOUR HUSBAND BUT YOU RE POSTING ANOTHER MAN FOR YOUR MAN CRUSH MONDAY S LOL : No desastre\n",
      "\n",
      "Los 10 tweets más negativos:\n",
      "SUICIDE BOMBER KILLS 15 IN SAUDI SECURITY SITE MOSQUE A SUICIDE BOMBER KILLED AT LEAST 15 PEOPLE IN AN ATTACK ON  : Desastre\n",
      "OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MARIANS A  : Desastre\n",
      "OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MARIANS A  : Desastre\n",
      " LOSDELSONIDO OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MA IVANBERROA  : Desastre\n",
      "OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MARIANS A  : Desastre\n",
      "OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MARIANS A  : Desastre\n",
      "OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MARIANS A  : Desastre\n",
      " LOSDELSONIDO OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARATION FOR NORTHERN MA IVANBERROA  : Desastre\n",
      "PHOTOS 17 PEOPLE KILLED AND OVER 25 INJURED IN DEADLY SAUDI MOSQUE SUICIDE ATTACK  : Desastre\n",
      " BREAKING144 OBAMA DECLARES DISASTER FOR TYPHOON DEVASTATED SAIPAN OBAMA SIGNS DISASTER DECLARAT ACENEWSDESK : Desastre\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6411    None\n",
       "7035    None\n",
       "7038    None\n",
       "2719    None\n",
       "7024    None\n",
       "7023    None\n",
       "7021    None\n",
       "7034    None\n",
       "4540    None\n",
       "7017    None\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_type(row):\n",
    "    print(f\"{row['text']} : {'Desastre' if row['target'] == 1 else 'No desastre'}\")\n",
    "\n",
    "print(\"Los 10 tweets más positivos:\")\n",
    "positive_tweets = tweets.tail(10)\n",
    "positive_tweets.apply(print_type, axis=1)\n",
    "\n",
    "print(\"\\nLos 10 tweets más negativos:\")\n",
    "negative_tweets = tweets.head(10)\n",
    "negative_tweets.apply(print_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.layers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer including bigrams and trigrams\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=5000)\n",
    "\n",
    "X = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, data['target'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7931713722915299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83       874\n",
      "           1       0.84      0.64      0.72       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.77      0.78      1523\n",
      "weighted avg       0.80      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modelo 4: Random Forest con TF-IDF\n",
    "model4 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model4.fit(X_train, y_train)\n",
    "y_pred = model4.predict(X_val)\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The house is on fire - Disaster\n",
      "I am happy - Non-disaster\n"
     ]
    }
   ],
   "source": [
    "def is_disaster(tweet):\n",
    "    tweet_transformed = vectorizer.transform(tweet)\n",
    "    return model4.predict(tweet_transformed)[0]\n",
    "\n",
    "tweet = ['The house is on fire', 'I am happy']\n",
    "\n",
    "for t in tweet:\n",
    "    print(f'{t} - {\"Disaster\" if is_disaster([t]) == 1 else \"Non-disaster\"}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
